{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (4.32.1)\n",
      "Requirement already satisfied: bert-serving-client in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (1.9.6)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/.local/lib/python3.6/site-packages (from bert-serving-client) (1.17.1)\n",
      "Requirement already satisfied: pyzmq>=17.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from bert-serving-client) (18.1.0)\n",
      "\u001b[31mfastai 1.0.55 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "\u001b[31mtensorboard 1.14.0 has requirement protobuf>=3.6.0, but you'll have protobuf 3.5.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mtensorboard 1.14.0 has requirement setuptools>=41.0.0, but you'll have setuptools 39.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mthinc 6.12.1 has requirement msgpack<0.6.0,>=0.5.6, but you'll have msgpack 0.6.0 which is incompatible.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm bert-serving-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::579019700964:role/service-role/AmazonSageMaker-ExecutionRole-20190429T111678\n",
      "sagemaker-us-east-1-579019700964\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3\n",
    "import numpy as np\n",
    "import re\n",
    "import tqdm\n",
    "import pickle \n",
    "import pandas as pd\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "print(role) # This is the role that SageMaker would use to leverage AWS resources (S3, CloudWatch) on your behalf\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "print(bucket)\n",
    "s3_output_location = 's3://{}/nlp/cnnoutput'.format(bucket) # This is the output location for models\n",
    "prefix = 'ML_workshop' #Replace with the prefix under which you want to store the data if needed\n",
    "region_name = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'默认分组': '__label__0',\n",
       " '结算问题（商管专用）': '__label__1',\n",
       " '无效咨询（在线专用）': '__label__2',\n",
       " '临时问题（在线专用）': '__label__3',\n",
       " '订单问题（商管专用）': '__label__4',\n",
       " '财务问题（商管专用）': '__label__5',\n",
       " '规则问题（商管专用）': '__label__6',\n",
       " '售后问题（在线专用）': '__label__7',\n",
       " '售后问题（商管专用）': '__label__8',\n",
       " 'plus分销': '__label__9',\n",
       " '商品问题（商管专用）': '__label__10',\n",
       " '店铺问题（商管专用）': '__label__11',\n",
       " '售中问题（在线专用）': '__label__12',\n",
       " '售前问题（在线专用）': '__label__13',\n",
       " '社群专用': '__label__14',\n",
       " '入驻问题（商管专用）': '__label__15',\n",
       " '售后问题（蜜店宝）': '__label__16',\n",
       " '__label__0': '默认分组',\n",
       " '__label__1': '结算问题（商管专用）',\n",
       " '__label__2': '无效咨询（在线专用）',\n",
       " '__label__3': '临时问题（在线专用）',\n",
       " '__label__4': '订单问题（商管专用）',\n",
       " '__label__5': '财务问题（商管专用）',\n",
       " '__label__6': '规则问题（商管专用）',\n",
       " '__label__7': '售后问题（在线专用）',\n",
       " '__label__8': '售后问题（商管专用）',\n",
       " '__label__9': 'plus分销',\n",
       " '__label__10': '商品问题（商管专用）',\n",
       " '__label__11': '店铺问题（商管专用）',\n",
       " '__label__12': '售中问题（在线专用）',\n",
       " '__label__13': '售前问题（在线专用）',\n",
       " '__label__14': '社群专用',\n",
       " '__label__15': '入驻问题（商管专用）',\n",
       " '__label__16': '售后问题（蜜店宝）'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pk = open('class_mapping_r.pkl', \"rb\") \n",
    "index_to_label = pickle.load(pk)\n",
    "\n",
    "s3_train_data = pd.read_csv('data/train.tsv',header=None, delimiter='\\t')\n",
    "s3_dev_data = pd.read_csv('data/dev.tsv',header=None, delimiter='\\t')\n",
    "\n",
    "index_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = s3_train_data[1]\n",
    "train_raw = train_raw.apply(lambda x: x if len(x)> 0 else np.nan)\n",
    "train = train_raw.dropna()\n",
    "\n",
    "test_raw = s3_dev_data[1]\n",
    "test_raw = test_raw.apply(lambda x: x if len(x)> 0 else np.nan)\n",
    "test = test_raw.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding y\n",
    "import sklearn.preprocessing as pre\n",
    "y_raw=s3_train_data[0].astype('category')\n",
    "y = y_raw[train_raw.notnull()]\n",
    "# print(y_raw.head())\n",
    "encoder = pre.LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n",
    "# print(y)\n",
    "with open('data/train_y.pkl', 'wb') as f:\n",
    "    pickle.dump(y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-serving-server\n",
      "  Using cached https://files.pythonhosted.org/packages/af/ef/0df9a6ce54a02d0a891d25af60e49b3a3a64d425e80c28acfee97f5ab0f2/bert_serving_server-1.9.6-py3-none-any.whl\n",
      "Collecting bert-serving-client\n",
      "  Using cached https://files.pythonhosted.org/packages/b5/2f/dd50af5b8dbde79e69f4bd2edec222eaa23d1015b03e9613411b78f9a639/bert_serving_client-1.9.6-py2.py3-none-any.whl\n",
      "Requirement not upgraded as not directly required: termcolor>=1.1 in /home/ec2-user/.local/lib/python3.6/site-packages (from bert-serving-server) (1.1.0)\n",
      "Requirement not upgraded as not directly required: numpy in /home/ec2-user/.local/lib/python3.6/site-packages (from bert-serving-server) (1.17.1)\n",
      "Collecting pyzmq>=17.1.0 (from bert-serving-server)\n",
      "  Using cached https://files.pythonhosted.org/packages/75/89/6f0ea51ffa9c2c00c0ab0460f137b16a5ab5b47e3b060c5b1fc9ca425836/pyzmq-18.1.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting GPUtil>=1.3.0 (from bert-serving-server)\n",
      "Requirement not upgraded as not directly required: six in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from bert-serving-server) (1.11.0)\n",
      "Installing collected packages: pyzmq, GPUtil, bert-serving-server, bert-serving-client\n",
      "  Found existing installation: pyzmq 17.0.0\n",
      "    Uninstalling pyzmq-17.0.0:\n",
      "      Successfully uninstalled pyzmq-17.0.0\n",
      "Successfully installed GPUtil-1.4.0 bert-serving-client-1.9.6 bert-serving-server-1.9.6 pyzmq-18.1.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Do the following operation in a EC2\n",
    "# !pip install -U bert-serving-server bert-serving-client\n",
    "# !wget https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\n",
    "# !unzip chinese_L-12_H-768_A-12.zip\n",
    "# !bert-serving-start -model_dir /tmp/chinese_L-12_H-768_A-12/ -num_worker=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/4250 [00:00<05:49, 12.13it/s]/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/bert_serving/client/__init__.py:299: UserWarning: some of your sentences have more tokens than \"max_seq_len=25\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
      "here is what you can do:\n",
      "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
      "- or, start a new server with a larger \"max_seq_len\"\n",
      "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n",
      "100%|██████████| 4250/4250 [06:26<00:00, 11.00it/s]\n",
      "100%|██████████| 473/473 [00:43<00:00, 10.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# Bert Preprocessing and save datasets.\n",
    "\n",
    "from bert_serving.client import BertClient\n",
    "bc = BertClient(ip='18.224.55.190') # this ip would change\n",
    "train_X = []\n",
    "for sentence in tqdm.tqdm(train.values):\n",
    "    lst = [ x for x in re.split('。|，|！',sentence) if len(x)>2 ]\n",
    "    train_X.append(bc.encode(lst))\n",
    "\n",
    " # write train_X\n",
    "with open('data/train_X.pkl', 'wb') as f:\n",
    "    pickle.dump(train_X, f)\n",
    "\n",
    "test_X = []\n",
    "for sentence in tqdm.tqdm(test.values):\n",
    "    lst = [ x for x in re.split('。|，|！',sentence) if len(x)>2 ]\n",
    "    test_X.append(bc.encode(lst))\n",
    "with open('data/test_X.pkl', 'wb') as f:\n",
    "    pickle.dump(test_X, f)\n",
    "test_y = s3_train_data[0]\n",
    "with open('data/test_y.pkl', 'wb') as f:\n",
    "    pickle.dump(test_y, f)\n",
    "\n",
    "    \n",
    "# If the server not working:\n",
    "# !wget https://sagemaker-us-east-2-069799604450.s3.us-east-2.amazonaws.com/ML_workshop/test_X.pkl\n",
    "# !wget https://sagemaker-us-east-2-069799604450.s3.us-east-2.amazonaws.com/ML_workshop/test_y.pkl\n",
    "# !wget https://sagemaker-us-east-2-069799604450.s3.us-east-2.amazonaws.com/ML_workshop/train_X.pkl\n",
    "# !wget https://sagemaker-us-east-2-069799604450.s3.us-east-2.amazonaws.com/ML_workshop/train_y.pkl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec (in this case, just an S3 path): s3://sagemaker-us-east-1-579019700964/ML_workshop\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path='/home/ec2-user/SageMaker/gcr_sm_workshop/NLP/NLP_DL/data/', bucket=bucket, key_prefix=prefix)\n",
    "print('input spec (in this case, just an S3 path): {}'.format(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-06 02:51:11 Starting - Starting the training job...\n",
      "2019-09-06 02:51:12 Starting - Launching requested ML instances...\n",
      "2019-09-06 02:52:11 Starting - Preparing the instances for training......\n",
      "2019-09-06 02:53:03 Downloading - Downloading input data\n",
      "2019-09-06 02:53:03 Training - Downloading the training image..\n",
      "\u001b[31mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[31mbash: no job control in this shell\u001b[0m\n",
      "\u001b[31m2019-09-06 02:53:15,840 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[31m2019-09-06 02:53:15,843 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m2019-09-06 02:53:15,852 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[31m2019-09-06 02:53:22,098 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[31m2019-09-06 02:53:22,288 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[31mGenerating setup.py\u001b[0m\n",
      "\u001b[31m2019-09-06 02:53:22,288 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[31m2019-09-06 02:53:22,288 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[31m2019-09-06 02:53:22,289 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m pip install . \u001b[0m\n",
      "\u001b[31mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[31mBuilding wheels for collected packages: train\n",
      "  Running setup.py bdist_wheel for train: started\n",
      "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-bdiq4o84/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[31mSuccessfully built train\u001b[0m\n",
      "\u001b[31mInstalling collected packages: train\u001b[0m\n",
      "\u001b[31mSuccessfully installed train-1.0.0\u001b[0m\n",
      "\u001b[31mYou are using pip version 18.1, however version 19.2.3 is available.\u001b[0m\n",
      "\u001b[31mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31m2019-09-06 02:53:23,493 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m2019-09-06 02:53:23,503 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[31mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[31m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batchsize\": 128,\n",
      "        \"epochs\": 10\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-pytorch-2019-09-06-02-51-10-792\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-579019700964/sagemaker-pytorch-2019-09-06-02-51-10-792/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[31m}\n",
      "\u001b[0m\n",
      "\u001b[31mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[31mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[31mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[31mSM_HPS={\"batchsize\":128,\"epochs\":10}\u001b[0m\n",
      "\u001b[31mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[31mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[31mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[31mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[31mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[31mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31mSM_NUM_CPUS=16\u001b[0m\n",
      "\u001b[31mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[31mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31mSM_MODULE_DIR=s3://sagemaker-us-east-1-579019700964/sagemaker-pytorch-2019-09-06-02-51-10-792/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[31mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batchsize\":128,\"epochs\":10},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-pytorch-2019-09-06-02-51-10-792\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-579019700964/sagemaker-pytorch-2019-09-06-02-51-10-792/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":16,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[31mSM_USER_ARGS=[\"--batchsize\",\"128\",\"--epochs\",\"10\"]\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[31mSM_HP_BATCHSIZE=128\u001b[0m\n",
      "\u001b[31mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[31mPYTHONPATH=/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[31mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m train --batchsize 128 --epochs 10\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31mepoch:  0\u001b[0m\n",
      "\n",
      "2019-09-06 02:53:15 Training - Training image download completed. Training in progress.\u001b[31mepochaccuracy:  tensor(10.2906)\u001b[0m\n",
      "\u001b[31mepochloss:  6.662617949878468\u001b[0m\n",
      "\u001b[31mepoch:  1\u001b[0m\n",
      "\u001b[31mepochaccuracy:  tensor(29.6893)\u001b[0m\n",
      "\u001b[31mepochloss:  2.206540521453409\u001b[0m\n",
      "\u001b[31mepoch:  2\u001b[0m\n",
      "\u001b[31mepochaccuracy:  tensor(33.9649)\u001b[0m\n",
      "\u001b[31mepochloss:  1.7619815153234146\u001b[0m\n",
      "\u001b[31mepoch:  3\u001b[0m\n",
      "\u001b[31mepochaccuracy:  tensor(35.3613)\u001b[0m\n",
      "\u001b[31mepochloss:  1.6759201568715714\u001b[0m\n",
      "\u001b[31mepoch:  4\u001b[0m\n",
      "\u001b[31mepochaccuracy:  tensor(35.7979)\u001b[0m\n",
      "\u001b[31mepochloss:  1.6071994269595427\u001b[0m\n",
      "\u001b[31mepoch:  5\u001b[0m\n",
      "\u001b[31mepochaccuracy:  tensor(37.3851)\u001b[0m\n",
      "\u001b[31mepochloss:  1.5742679273380953\u001b[0m\n",
      "\u001b[31mepoch:  6\u001b[0m\n",
      "\u001b[31mepochaccuracy:  tensor(37.1394)\u001b[0m\n",
      "\u001b[31mepochloss:  1.5528876080232508\u001b[0m\n",
      "\u001b[31mepoch:  7\u001b[0m\n",
      "\u001b[31mepochaccuracy:  tensor(37.1359)\u001b[0m\n",
      "\u001b[31mepochloss:  1.5115670737098246\u001b[0m\n",
      "\u001b[31mepoch:  8\u001b[0m\n",
      "\u001b[31mepochaccuracy:  tensor(37.1270)\u001b[0m\n",
      "\u001b[31mepochloss:  1.6623510017114527\u001b[0m\n",
      "\u001b[31mepoch:  9\u001b[0m\n",
      "\u001b[31mepochaccuracy:  tensor(33.9649)\u001b[0m\n",
      "\u001b[31mepochloss:  1.548096453442293\u001b[0m\n",
      "\u001b[31mFinished.\u001b[0m\n",
      "\u001b[31m2019-09-06 02:58:54,633 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2019-09-06 02:59:03 Uploading - Uploading generated training model\n",
      "2019-09-06 02:59:03 Completed - Training job completed\n",
      "Billable seconds: 367\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "estimator = PyTorch(entry_point='train.py',\n",
    "                    role=role,\n",
    "                    framework_version='1.1.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.c5.4xlarge',\n",
    "                    hyperparameters={\n",
    "                        'epochs': 10,\n",
    "                        'batchsize': 128\n",
    "                    }\n",
    "                    )\n",
    "estimator.fit({'training': inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-579019700964/sagemaker-pytorch-2019-09-06-02-51-10-792/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "#check model\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "training_job_name = estimator.latest_training_job.name\n",
    "desc = sagemaker_session.sagemaker_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "trained_model_location = desc['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(trained_model_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-579019700964/sagemaker-pytorch-2019-09-06-02-51-10-792/output/model.tar.gz to ./model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp $trained_model_location .\n",
    "!tar xf model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: __main__.py [-h] [--batchsize BATCHSIZE] [--epochs EPOCHS]\n",
      "__main__.py: error: unrecognized arguments: -f /home/ec2-user/.local/share/jupyter/runtime/kernel-972ee2a1-e431-48c9-88f2-c59799b4fa22.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from train import *\n",
    "new_model=torch.load('model.pth')\n",
    "test_label = []\n",
    "test_X = pickle.load(open('data/test_X.pkl', 'rb'))\n",
    "for index in range(len(test_X)):\n",
    "    test=torch.as_tensor(test_X[index]).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        result = new_model(test)\n",
    "        label = result[0].argmax(dim=-1).numpy()[0].astype(int)\n",
    "        print('label{}: '.format(index),index_to_label[encoder.inverse_transform([label])[0]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__label__15', '__label__9'}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
